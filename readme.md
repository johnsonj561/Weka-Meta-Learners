A data set which describes software modules and labels each module as fault prone or non-fault prone is used to train and evaluate bagging and boosting meta learners. Bagging and boosting is applied to both the J48 decision tree (strong learner) and the Decision stump tree (weak learner). Through WEKA's cost sensitive classifier the Type I Type II error cost ratio is varied, and models are trained using the training data set (188 instances). For all examples, Type I misclassification cost is set to 1, and Type II misclassification cost is varied between 0.5 and 10. The optimal cost ratio is identified by selecting the model whose cross validation resulted in a Type I and Type II error rate that are approximately the same with a Type II error rate as low as possible. Finally, all models are evaluated against unfamiliar test data (94 instances). Classification results are also compared to the Pruned Tree model from the previous assignment.

Both bagging and boosting algorithms strive to increase classification accuracy by creating multiple models and combining their results to produce predictions on new data. For this assignment, bagging and boosting is first completed using a default of 10 iterations/models (Parts II - V), and then all steps are repeated using 25 iterations/models (Parts VI - IX).

The bagging ensemble algorithm constructs multiple training sets from the original training data set by randomly sampling the set with re-substitution. It then trains all of the models and produces a final classification result by combining the results of all individual models. In bagging algorithms, each model is given an equal weight, and the models vote on the final classification result. The label that receives the greatest number of votes is selected as the new instances class. 

Boosting algorithms are similar to bagging as they too construct multiple models and vote on new instance classification. Unlike bagging however, boosting algorithms construct models iteratively, using previous models to influence future models. The AdaBoost boosting algorithm applies weights to each instance in an iterative manner, using the new weights to construct new models which will complement previous models. Instances that are misclassified are given a greater weight, forcing the next iteration's model to focus more on instances previously misclassified.